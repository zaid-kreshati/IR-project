{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecafe5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['admin', 'config', 'local']\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# Connect to MongoDB running locally\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"ir_project\"]\n",
    "\n",
    "# Test: list databases (should be empty now)\n",
    "print(client.list_database_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c034dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522931\n",
      "{'_id': ObjectId('685d7c1806d7f0da4f93d96d'), 'doc_id': '1', 'body': 'What is the step by step guide to invest in share market in india?', 'clean_body': 'step step guide invest share market india', 'dataset': 'quora'}\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"ir_project\"]\n",
    "collection = db[\"documents\"]\n",
    "\n",
    "print(collection.count_documents({}))  # ÿπÿØÿØ ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™\n",
    "print(collection.find_one())           # ÿ£ŸàŸÑ ŸÖÿ≥ÿ™ŸÜÿØ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99f8a05b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/tfidf_vectorizer.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m vectorizer = \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels/tfidf_vectorizer.joblib\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m matrix = load(\u001b[33m\"\u001b[39m\u001b[33mmodels/tfidf_matrix.joblib\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m vectorizer.get_feature_names_out()[:\u001b[32m10\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/IR-project/venv/lib/python3.13/site-packages/joblib/numpy_pickle.py:735\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filename, mmap_mode, ensure_native_byte_order)\u001b[39m\n\u001b[32m    733\u001b[39m         obj = _unpickle(fobj, ensure_native_byte_order=ensure_native_byte_order)\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    736\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m _validate_fileobject_and_memmap(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m (\n\u001b[32m    737\u001b[39m             fobj,\n\u001b[32m    738\u001b[39m             validated_mmap_mode,\n\u001b[32m    739\u001b[39m         ):\n\u001b[32m    740\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    741\u001b[39m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[32m    742\u001b[39m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[32m    743\u001b[39m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'models/tfidf_vectorizer.joblib'"
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "vectorizer = load(\"models/tfidf_vectorizer.joblib\")\n",
    "matrix = load(\"models/tfidf_matrix.joblib\")\n",
    "\n",
    "vectorizer.get_feature_names_out()[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae207082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ File exists: False\n",
      "üìÅ Absolute path: /Users/STADIA_AD/Desktop/IR-project/notebooks/tests/models/tfidf_vectorizer.joblib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"models/tfidf_vectorizer.joblib\"\n",
    "print(\"üìÑ File exists:\", os.path.exists(path))\n",
    "print(\"üìÅ Absolute path:\", os.path.abspath(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f33dac0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'app'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m vectorizer_path = os.path.join(project_root, \u001b[33m\"\u001b[39m\u001b[33mmodels/tfidf_vectorizer.joblib\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m matrix_path = os.path.join(project_root, \u001b[33m\"\u001b[39m\u001b[33mmodels/tfidf_matrix.joblib\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m vectorizer = \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectorizer_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m matrix = load(matrix_path)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Loaded vectorizer and matrix successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/IR-project/venv/lib/python3.13/site-packages/joblib/numpy_pickle.py:749\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filename, mmap_mode, ensure_native_byte_order)\u001b[39m\n\u001b[32m    744\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[32m    746\u001b[39m             \u001b[38;5;66;03m# A memory-mapped array has to be mapped with the endianness\u001b[39;00m\n\u001b[32m    747\u001b[39m             \u001b[38;5;66;03m# it has been written with. Other arrays are coerced to the\u001b[39;00m\n\u001b[32m    748\u001b[39m             \u001b[38;5;66;03m# native endianness of the host system.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m             obj = \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m                \u001b[49m\u001b[43mensure_native_byte_order\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_native_byte_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidated_mmap_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/IR-project/venv/lib/python3.13/site-packages/joblib/numpy_pickle.py:626\u001b[39m, in \u001b[36m_unpickle\u001b[39m\u001b[34m(fobj, ensure_native_byte_order, filename, mmap_mode)\u001b[39m\n\u001b[32m    624\u001b[39m obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     obj = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    627\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m unpickler.compat_mode:\n\u001b[32m    628\u001b[39m         warnings.warn(\n\u001b[32m    629\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe file \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has been generated with a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    630\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mjoblib version less than 0.10. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    633\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    634\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.0/lib/python3.13/pickle.py:1255\u001b[39m, in \u001b[36m_Unpickler.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1253\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[32m   1254\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[32m-> \u001b[39m\u001b[32m1255\u001b[39m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1256\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[32m   1257\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst.value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.0/lib/python3.13/pickle.py:1580\u001b[39m, in \u001b[36m_Unpickler.load_stack_global\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(name) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(module) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m   1579\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\u001b[33m\"\u001b[39m\u001b[33mSTACK_GLOBAL requires str\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1580\u001b[39m \u001b[38;5;28mself\u001b[39m.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.0/lib/python3.13/pickle.py:1621\u001b[39m, in \u001b[36m_Unpickler.find_class\u001b[39m\u001b[34m(self, module, name)\u001b[39m\n\u001b[32m   1619\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m _compat_pickle.IMPORT_MAPPING:\n\u001b[32m   1620\u001b[39m         module = _compat_pickle.IMPORT_MAPPING[module]\n\u001b[32m-> \u001b[39m\u001b[32m1621\u001b[39m \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.proto >= \u001b[32m4\u001b[39m:\n\u001b[32m   1623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _getattribute(sys.modules[module], name)[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'app'"
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))  # go up 2 levels\n",
    "vectorizer_path = os.path.join(project_root, \"models/tfidf_vectorizer.joblib\")\n",
    "matrix_path = os.path.join(project_root, \"models/tfidf_matrix.joblib\")\n",
    "\n",
    "vectorizer = load(vectorizer_path)\n",
    "matrix = load(matrix_path)\n",
    "\n",
    "print(\"‚úÖ Loaded vectorizer and matrix successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "935700fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Go up from `notebooks/tests` to project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.append(project_root)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9e1e9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded. Matrix shape: (522931, 63230)\n"
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "\n",
    "vectorizer = load(os.path.join(project_root, \"models/tfidf_vectorizer.joblib\"))\n",
    "matrix = load(os.path.join(project_root, \"models/tfidf_matrix.joblib\"))\n",
    "print(\"‚úÖ Loaded. Matrix shape:\", matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0231de54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_b_2_x_c_\n"
     ]
    }
   ],
   "source": [
    "name =\"a/b/2/x/c/\"\n",
    "name=name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").strip()\n",
    "print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca2d4fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382545\n"
     ]
    }
   ],
   "source": [
    "from ir_datasets import load\n",
    "ds = load(\"beir/webis-touche2020\")\n",
    "print(ds.docs_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c3ffab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "idf values:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TfidfVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# get idf values\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33midf values:\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ele1, ele2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mtfidf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_feature_names\u001b[49m(), tfidf.idf_):\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(ele1, \u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m'\u001b[39m, ele2)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# get indexing\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'TfidfVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "# import required module\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# assign documents\n",
    "d0 = 'Geeks for geeks'\n",
    "d1 = 'Geeks'\n",
    "d2 = 'r2j'\n",
    "\n",
    "# merge documents into a single corpus\n",
    "string = [d0, d1, d2]\n",
    "\n",
    "# create object\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# get tf-df values\n",
    "result = tfidf.fit_transform(string)\n",
    "\n",
    "# get idf values\n",
    "print('\\nidf values:')\n",
    "for ele1, ele2 in zip(tfidf.get_feature_names(), tfidf.idf_):\n",
    "    print(ele1, ':', ele2)\n",
    "\n",
    "# get indexing\n",
    "print('\\nWord indexes:')\n",
    "print(tfidf.vocabulary_)\n",
    "\n",
    "# display tf-idf values\n",
    "print('\\ntf-idf value:')\n",
    "print(result)\n",
    "\n",
    "# in matrix form\n",
    "print('\\ntf-idf values in matrix form:')\n",
    "print(result.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d1a34f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word indexes:\n",
      "{'geek1': 0, 'geek2': 1, 'geek3': 2, 'geek4': 3}\n",
      "\n",
      "tf-idf values:\n",
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 4 stored elements and shape (4, 4)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 2)\t1.0\n",
      "  (3, 3)\t1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import required module\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# assign documents\n",
    "d0 = 'geek1'\n",
    "d1 = 'geek2'\n",
    "d2 = 'geek3'\n",
    "d3 = 'geek4'\n",
    "\n",
    "# merge documents into a single corpus\n",
    "string = [d0, d1, d2, d3]\n",
    "\n",
    "# create object\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# get tf-df values\n",
    "result = tfidf.fit_transform(string)\n",
    "\n",
    "# get indexing\n",
    "print('\\nWord indexes:')\n",
    "print(tfidf.vocabulary_)\n",
    "\n",
    "# display tf-idf values\n",
    "print('\\ntf-idf values:')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7a3abab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word indexes:\n",
      "{'geeks': 1, 'for': 0}\n",
      "\n",
      "tf-idf values:\n",
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 4 stored elements and shape (2, 2)>\n",
      "  Coords\tValues\n",
      "  (0, 1)\t0.8944271909999159\n",
      "  (0, 0)\t0.4472135954999579\n",
      "  (1, 1)\t0.8944271909999159\n",
      "  (1, 0)\t0.4472135954999579\n"
     ]
    }
   ],
   "source": [
    "# import required module\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# assign documents\n",
    "d0 = 'Geeks for geeks!'\n",
    "d1 = 'Geeks for geeks!'\n",
    "\n",
    "\n",
    "# merge documents into a single corpus\n",
    "string = [d0, d1]\n",
    "\n",
    "# create object\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# get tf-df values\n",
    "result = tfidf.fit_transform(string)\n",
    "\n",
    "# get indexing\n",
    "print('\\nWord indexes:')\n",
    "print(tfidf.vocabulary_)\n",
    "\n",
    "# display tf-idf values\n",
    "print('\\ntf-idf values:')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff22cb7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'app'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mapp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mservices\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocess\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m preprocess_text\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(preprocess_text(\u001b[33m\"\u001b[39m\u001b[33mSearch at the corner in the morning.\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'app'"
     ]
    }
   ],
   "source": [
    "from app.services.preprocess import preprocess_text\n",
    "\n",
    "print(preprocess_text(\"Search at the corner in the morning.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94d909c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('6863c9529ba1016f59a2edc5'), 'doc_id': 'test-environment-aeghhgwpe-pro01a', 'body': \"It is immoral to kill animals  As evolved human beings it is our moral duty to inflict as little pain as possible for our survival. So if we do not need to inflict pain to animals in order to survive, we should not do it. Farm animals such as chickens, pigs, sheep, and cows are sentient living beings like us - they are our evolutionary cousins and like us they can feel pleasure and pain. The 18th century utilitarian philosopher Jeremy Bentham even believed that animal suffering was just as serious as human suffering and likened the idea of human superiority to racism. It is wrong to farm and kill these animals for food when we do not need to do so. The methods of farming and slaughter of these animals are often barbaric and cruel - even on supposedly 'free range' farms. [1] Ten billion animals were slaughtered for human consumption each year, stated PETA. And unlike the farms long time ago, where animals roamed freely, today, most animals are factory farmed: \\x97crammed into cages where they can barely move and fed a diet adulterated with pesticides and antibiotics. These animals spend their entire lives in their ‚Äúprisoner cells‚Äù so small that they can't even turn around. Many suffer serious health problems and even death because they are selectively bred to grow or produce milk or eggs at a far greater rate than their bodies are capable of coping with. At the slaughterhouse, there were millions of others who are killed every year for food.  Further on Tom Regan explains that all duties regarding animals are indirect duties to one another from a philosophical point of view. He illustrates it with an analogy regarding children: ‚ÄúChildren, for example, are unable to sign contracts and lack rights. But they are protected by the moral contract nonetheless because of the sentimental interests of others. So we have, then, duties involving these children, duties regarding them, but no duties to them. Our duties in their case are indirect duties to other human beings, usually their parents.‚Äù [2] With this he supports the theory that animals must be protected from suffering, as it is moral to protect any living being from suffering, not because we have a moral contract with them, but mainly due to respect of life and recognition of suffering itself.  [1] Claire Suddath, A brief history of Veganism, Time, 30 October 2008  [2] Tom Regan, The case for animal rights, 1989\"}\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "collection = client[\"ir_project\"][\"nano-beir-arguana\"]\n",
    "sample = collection.find_one()\n",
    "print(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62990983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
