{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecafe5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['admin', 'config', 'local']\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# Connect to MongoDB running locally\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"ir_project\"]\n",
    "\n",
    "# Test: list databases (should be empty now)\n",
    "print(client.list_database_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c034dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522931\n",
      "{'_id': ObjectId('685d7c1806d7f0da4f93d96d'), 'doc_id': '1', 'body': 'What is the step by step guide to invest in share market in india?', 'clean_body': 'step step guide invest share market india', 'dataset': 'quora'}\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"ir_project\"]\n",
    "collection = db[\"documents\"]\n",
    "\n",
    "print(collection.count_documents({}))  # عدد المستندات\n",
    "print(collection.find_one())           # أول مستند\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99f8a05b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/tfidf_vectorizer.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m vectorizer = \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels/tfidf_vectorizer.joblib\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m matrix = load(\u001b[33m\"\u001b[39m\u001b[33mmodels/tfidf_matrix.joblib\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m vectorizer.get_feature_names_out()[:\u001b[32m10\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/IR-project/venv/lib/python3.13/site-packages/joblib/numpy_pickle.py:735\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filename, mmap_mode, ensure_native_byte_order)\u001b[39m\n\u001b[32m    733\u001b[39m         obj = _unpickle(fobj, ensure_native_byte_order=ensure_native_byte_order)\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    736\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m _validate_fileobject_and_memmap(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m (\n\u001b[32m    737\u001b[39m             fobj,\n\u001b[32m    738\u001b[39m             validated_mmap_mode,\n\u001b[32m    739\u001b[39m         ):\n\u001b[32m    740\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    741\u001b[39m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[32m    742\u001b[39m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[32m    743\u001b[39m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'models/tfidf_vectorizer.joblib'"
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "vectorizer = load(\"models/tfidf_vectorizer.joblib\")\n",
    "matrix = load(\"models/tfidf_matrix.joblib\")\n",
    "\n",
    "vectorizer.get_feature_names_out()[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae207082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 File exists: False\n",
      "📁 Absolute path: /Users/STADIA_AD/Desktop/IR-project/notebooks/tests/models/tfidf_vectorizer.joblib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"models/tfidf_vectorizer.joblib\"\n",
    "print(\"📄 File exists:\", os.path.exists(path))\n",
    "print(\"📁 Absolute path:\", os.path.abspath(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f33dac0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'app'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m vectorizer_path = os.path.join(project_root, \u001b[33m\"\u001b[39m\u001b[33mmodels/tfidf_vectorizer.joblib\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m matrix_path = os.path.join(project_root, \u001b[33m\"\u001b[39m\u001b[33mmodels/tfidf_matrix.joblib\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m vectorizer = \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectorizer_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m matrix = load(matrix_path)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Loaded vectorizer and matrix successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/IR-project/venv/lib/python3.13/site-packages/joblib/numpy_pickle.py:749\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filename, mmap_mode, ensure_native_byte_order)\u001b[39m\n\u001b[32m    744\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[32m    746\u001b[39m             \u001b[38;5;66;03m# A memory-mapped array has to be mapped with the endianness\u001b[39;00m\n\u001b[32m    747\u001b[39m             \u001b[38;5;66;03m# it has been written with. Other arrays are coerced to the\u001b[39;00m\n\u001b[32m    748\u001b[39m             \u001b[38;5;66;03m# native endianness of the host system.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m             obj = \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m                \u001b[49m\u001b[43mensure_native_byte_order\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_native_byte_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidated_mmap_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/IR-project/venv/lib/python3.13/site-packages/joblib/numpy_pickle.py:626\u001b[39m, in \u001b[36m_unpickle\u001b[39m\u001b[34m(fobj, ensure_native_byte_order, filename, mmap_mode)\u001b[39m\n\u001b[32m    624\u001b[39m obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     obj = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    627\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m unpickler.compat_mode:\n\u001b[32m    628\u001b[39m         warnings.warn(\n\u001b[32m    629\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe file \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has been generated with a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    630\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mjoblib version less than 0.10. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    633\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    634\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.0/lib/python3.13/pickle.py:1255\u001b[39m, in \u001b[36m_Unpickler.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1253\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[32m   1254\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[32m-> \u001b[39m\u001b[32m1255\u001b[39m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1256\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[32m   1257\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst.value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.0/lib/python3.13/pickle.py:1580\u001b[39m, in \u001b[36m_Unpickler.load_stack_global\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(name) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(module) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m   1579\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\u001b[33m\"\u001b[39m\u001b[33mSTACK_GLOBAL requires str\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1580\u001b[39m \u001b[38;5;28mself\u001b[39m.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.0/lib/python3.13/pickle.py:1621\u001b[39m, in \u001b[36m_Unpickler.find_class\u001b[39m\u001b[34m(self, module, name)\u001b[39m\n\u001b[32m   1619\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m _compat_pickle.IMPORT_MAPPING:\n\u001b[32m   1620\u001b[39m         module = _compat_pickle.IMPORT_MAPPING[module]\n\u001b[32m-> \u001b[39m\u001b[32m1621\u001b[39m \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.proto >= \u001b[32m4\u001b[39m:\n\u001b[32m   1623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _getattribute(sys.modules[module], name)[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'app'"
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))  # go up 2 levels\n",
    "vectorizer_path = os.path.join(project_root, \"models/tfidf_vectorizer.joblib\")\n",
    "matrix_path = os.path.join(project_root, \"models/tfidf_matrix.joblib\")\n",
    "\n",
    "vectorizer = load(vectorizer_path)\n",
    "matrix = load(matrix_path)\n",
    "\n",
    "print(\"✅ Loaded vectorizer and matrix successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "935700fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Go up from `notebooks/tests` to project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.append(project_root)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9e1e9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded. Matrix shape: (522931, 63230)\n"
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "\n",
    "vectorizer = load(os.path.join(project_root, \"models/tfidf_vectorizer.joblib\"))\n",
    "matrix = load(os.path.join(project_root, \"models/tfidf_matrix.joblib\"))\n",
    "print(\"✅ Loaded. Matrix shape:\", matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0231de54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_b_2_x_c_\n"
     ]
    }
   ],
   "source": [
    "name =\"a/b/2/x/c/\"\n",
    "name=name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").strip()\n",
    "print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca2d4fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382545\n"
     ]
    }
   ],
   "source": [
    "from ir_datasets import load\n",
    "ds = load(\"beir/webis-touche2020\")\n",
    "print(ds.docs_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c3ffab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "idf values:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TfidfVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# get idf values\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33midf values:\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ele1, ele2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mtfidf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_feature_names\u001b[49m(), tfidf.idf_):\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(ele1, \u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m'\u001b[39m, ele2)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# get indexing\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'TfidfVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "# import required module\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# assign documents\n",
    "d0 = 'Geeks for geeks'\n",
    "d1 = 'Geeks'\n",
    "d2 = 'r2j'\n",
    "\n",
    "# merge documents into a single corpus\n",
    "string = [d0, d1, d2]\n",
    "\n",
    "# create object\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# get tf-df values\n",
    "result = tfidf.fit_transform(string)\n",
    "\n",
    "# get idf values\n",
    "print('\\nidf values:')\n",
    "for ele1, ele2 in zip(tfidf.get_feature_names(), tfidf.idf_):\n",
    "    print(ele1, ':', ele2)\n",
    "\n",
    "# get indexing\n",
    "print('\\nWord indexes:')\n",
    "print(tfidf.vocabulary_)\n",
    "\n",
    "# display tf-idf values\n",
    "print('\\ntf-idf value:')\n",
    "print(result)\n",
    "\n",
    "# in matrix form\n",
    "print('\\ntf-idf values in matrix form:')\n",
    "print(result.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d1a34f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word indexes:\n",
      "{'geek1': 0, 'geek2': 1, 'geek3': 2, 'geek4': 3}\n",
      "\n",
      "tf-idf values:\n",
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 4 stored elements and shape (4, 4)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 2)\t1.0\n",
      "  (3, 3)\t1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import required module\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# assign documents\n",
    "d0 = 'geek1'\n",
    "d1 = 'geek2'\n",
    "d2 = 'geek3'\n",
    "d3 = 'geek4'\n",
    "\n",
    "# merge documents into a single corpus\n",
    "string = [d0, d1, d2, d3]\n",
    "\n",
    "# create object\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# get tf-df values\n",
    "result = tfidf.fit_transform(string)\n",
    "\n",
    "# get indexing\n",
    "print('\\nWord indexes:')\n",
    "print(tfidf.vocabulary_)\n",
    "\n",
    "# display tf-idf values\n",
    "print('\\ntf-idf values:')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7a3abab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word indexes:\n",
      "{'geeks': 1, 'for': 0}\n",
      "\n",
      "tf-idf values:\n",
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 4 stored elements and shape (2, 2)>\n",
      "  Coords\tValues\n",
      "  (0, 1)\t0.8944271909999159\n",
      "  (0, 0)\t0.4472135954999579\n",
      "  (1, 1)\t0.8944271909999159\n",
      "  (1, 0)\t0.4472135954999579\n"
     ]
    }
   ],
   "source": [
    "# import required module\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# assign documents\n",
    "d0 = 'Geeks for geeks!'\n",
    "d1 = 'Geeks for geeks!'\n",
    "\n",
    "\n",
    "# merge documents into a single corpus\n",
    "string = [d0, d1]\n",
    "\n",
    "# create object\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# get tf-df values\n",
    "result = tfidf.fit_transform(string)\n",
    "\n",
    "# get indexing\n",
    "print('\\nWord indexes:')\n",
    "print(tfidf.vocabulary_)\n",
    "\n",
    "# display tf-idf values\n",
    "print('\\ntf-idf values:')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff22cb7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'app'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mapp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mservices\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocess\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m preprocess_text\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(preprocess_text(\u001b[33m\"\u001b[39m\u001b[33mSearch at the corner in the morning.\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'app'"
     ]
    }
   ],
   "source": [
    "from app.services.preprocess import preprocess_text\n",
    "\n",
    "print(preprocess_text(\"Search at the corner in the morning.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d909c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
