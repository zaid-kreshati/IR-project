{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e2d5369",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ada179e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/STADIA_AD/Desktop/IR-project/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from app.services.evaluator import compute_recall,compute_precision,compute_average_precision,compute_precision_at_k,compute_mrr\n",
    "from app.services.hybrid_service import HybridSearchService\n",
    "import os\n",
    "import asyncio\n",
    "\n",
    "\n",
    "dataset_name1='nano-beir/arguana'\n",
    "dataset_name2='beir/webis-touche2020/v2'\n",
    "dataset_name3='beir/quora/test'\n",
    "dataset_name4='antique/test'\n",
    "\n",
    "datasetname=dataset_name4\n",
    "name=datasetname.replace(\"/\", \"-\").replace(\"\\\\\", \"_\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc7f7d",
   "metadata": {},
   "source": [
    "## Load queries and qrels files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51e746ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not datasetname:\n",
    "    raise ValueError(\"datasetname variable is not defined\")\n",
    "\n",
    "qrels_df = pd.read_csv(f\"data/{name}/qrels.tsv\", sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"])\n",
    "# print(qrels_df)\n",
    "queries_df = pd.read_csv(f\"data/{name}/queries.tsv\", sep=\"\\t\", names=[\"query_id\", \"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bbdc10",
   "metadata": {},
   "source": [
    "# make instance from Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dffcbf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS backend is available.\n",
      "Loading models for collection: antique-test\n",
      "‚ö†Ô∏è Loading BM25 model for collection: antique-test\n",
      "üîç Loading embeddings documents\n"
     ]
    }
   ],
   "source": [
    "hybrid=HybridSearchService(collection_name=name)\n",
    "await hybrid.load_models()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc435b9b",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57de8cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  25%|‚ñà‚ñà‚ñå       | 1/4 [02:57<08:51, 177.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Progress: 25.00% (50/200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [05:06<04:58, 149.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Progress: 50.00% (100/200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [06:56<02:11, 131.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Progress: 75.00% (150/200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [09:17<00:00, 139.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Progress: 100.00% (200/200)\n",
      "\n",
      "‚úÖ MAP: 0.2139\n",
      "‚úÖ MRR: 0.8752\n",
      "‚úÖ Mean Recall: 0.6267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to list.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ all Precision@10: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mall_prec_at_10\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Now run it in Jupyter:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ MRR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmrr_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Mean Recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(all_recall)/\u001b[38;5;28mlen\u001b[39m(all_recall)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ all Precision@10: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mall_prec_at_10\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: unsupported format string passed to list.__format__"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import asyncio\n",
    "\n",
    "BATCH_SIZE = 50  # Tune this for your machine ‚Äî higher is faster but needs more memory\n",
    "\n",
    "async def evaluate_query(row):\n",
    "    query_id = row[\"query_id\"]\n",
    "    query_text = row[\"text\"]\n",
    "\n",
    "    search_results = await hybrid.search_with_Index(query=query_text, top_k=1000)\n",
    "    result_items = search_results[\"results\"]\n",
    "    retrieved_docs = [str(doc[\"doc_id\"]) for doc in result_items]\n",
    "    relevant_docs = set(qrels_df[qrels_df[\"query_id\"] == query_id][\"doc_id\"].astype(str))\n",
    "\n",
    "    avg_precision = compute_average_precision(relevant_docs, retrieved_docs)\n",
    "    prec_at_10 = compute_precision_at_k(relevant_docs, retrieved_docs, k=10)\n",
    "    recall = compute_recall(relevant_docs, retrieved_docs)\n",
    "\n",
    "    rank = 0\n",
    "    for i, doc_id in enumerate(retrieved_docs):\n",
    "        if doc_id in relevant_docs:\n",
    "            rank = i + 1\n",
    "            break\n",
    "\n",
    "    return avg_precision, prec_at_10, recall, rank\n",
    "\n",
    "\n",
    "async def main():\n",
    "    all_avg_precisions = []\n",
    "    all_prec_at_10 = []\n",
    "    all_mrr_ranks = []\n",
    "    all_recall = []\n",
    "\n",
    "    rows = list(queries_df.iterrows())\n",
    "\n",
    "    for i in tqdm(range(0, len(rows), BATCH_SIZE), desc=\"Evaluating\"):\n",
    "        batch = rows[i:i+BATCH_SIZE]\n",
    "        tasks = [evaluate_query(row) for _, row in batch]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        total = len(rows)\n",
    "\n",
    "        for avg_precision, prec_at_10, recall, rank in results:\n",
    "            all_avg_precisions.append(avg_precision)\n",
    "            all_prec_at_10.append(prec_at_10)\n",
    "            all_mrr_ranks.append(rank)\n",
    "            all_recall.append(recall)\n",
    "\n",
    "        percent = (i + len(batch)) / total * 100\n",
    "        print(f\"‚úÖ Progress: {percent:.2f}% ({i + len(batch)}/{total})\")\n",
    "\n",
    "    # Final results\n",
    "    map_score = sum(all_avg_precisions) / len(all_avg_precisions)\n",
    "    mrr_score = compute_mrr(all_mrr_ranks)\n",
    "\n",
    "    print(f\"\\n‚úÖ MAP: {map_score:.4f}\")\n",
    "    print(f\"‚úÖ MRR: {mrr_score:.4f}\")\n",
    "    print(f\"‚úÖ Mean Recall: {sum(all_recall)/len(all_recall):.4f}\")\n",
    "    print(f\"‚úÖ all Precision@10: {all_prec_at_10:.4f}\")\n",
    "\n",
    "# Now run it in Jupyter:\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5990ae6",
   "metadata": {},
   "source": [
    "## save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b03a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    \"Mean Average Precision\": map_score,\n",
    "    \"Mean Reciprocal Rank\": mrr_score\n",
    "}\n",
    "import json\n",
    "\n",
    "output_dir = os.path.join(\"results\", \"HYBRID\", name)\n",
    "os.makedirs(output_dir)\n",
    "output_path = os.path.join(output_dir,\"evaluation_summary.json\")\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(summary, f)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d98e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame({'All Recall value': all_recall, 'All Precision@10 values': all_prec_at_10})\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Convert string path to Path object\n",
    "output_dir = Path(\"results/Hybrid/\")/name\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save as TSV\n",
    "df.to_csv(output_dir / 'evaluation.tsv', sep='\\t', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601b89ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
